{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to load data from directories\n",
    "def load_data(file_path):\n",
    "    column_names = ['Time (s)', 'X (m/s2)', 'Y (m/s2)', 'Z (m/s2)', 'R (m/s2)', 'Theta (deg)', 'Phi (deg)']\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()[4:]  # Skip metadata lines\n",
    "        data = [line.strip().split(',') for line in lines if len(line.strip().split(',')) == len(column_names)]\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')  # Convert to numeric\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_parse_data(base_dir):\n",
    "    subjects = os.listdir(base_dir)\n",
    "    parsed_data = {}\n",
    "    \n",
    "    for subject in subjects:\n",
    "        subject_dir = os.path.join(base_dir, subject)\n",
    "        joints = os.listdir(subject_dir)\n",
    "        parsed_data[subject] = {}\n",
    "        \n",
    "        for joint_file in joints:\n",
    "            joint = os.path.splitext(joint_file)[0]\n",
    "            file_path = os.path.join(subject_dir, joint_file)\n",
    "            print(f\"Reading file: {file_path}\")\n",
    "            \n",
    "            try:\n",
    "                joint_data = load_data(file_path)\n",
    "                # print(f\"Processed data:\\n{joint_data.head()}\")\n",
    "                # print(f\"Data shape: {joint_data.shape}\")\n",
    "                \n",
    "                parsed_data[subject][joint] = joint_data\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_path}: {e}\")\n",
    "    \n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: Final_Dataset/Abhay/LK.csv\n",
      "Reading file: Final_Dataset/Abhay/LH.csv\n",
      "Reading file: Final_Dataset/Abhay/RH.csv\n",
      "Reading file: Final_Dataset/Abhay/RK.csv\n",
      "Reading file: Final_Dataset/Abhay/RA.csv\n",
      "Reading file: Final_Dataset/Abhay/LA.csv\n",
      "Reading file: Final_Dataset/Kavya/LK.csv\n",
      "Reading file: Final_Dataset/Kavya/LH.csv\n",
      "Reading file: Final_Dataset/Kavya/RH.csv\n",
      "Reading file: Final_Dataset/Kavya/RK.csv\n",
      "Reading file: Final_Dataset/Kavya/RA.csv\n",
      "Reading file: Final_Dataset/Kavya/LA.csv\n",
      "Reading file: Final_Dataset/Gaavya/LK.csv\n",
      "Reading file: Final_Dataset/Gaavya/LH.csv\n",
      "Reading file: Final_Dataset/Gaavya/RH.csv\n",
      "Reading file: Final_Dataset/Gaavya/RK.csv\n",
      "Reading file: Final_Dataset/Gaavya/RA.csv\n",
      "Reading file: Final_Dataset/Gaavya/LA.csv\n",
      "Reading file: Final_Dataset/Vishnu/LK.csv\n",
      "Reading file: Final_Dataset/Vishnu/LH.csv\n",
      "Reading file: Final_Dataset/Vishnu/RH.csv\n",
      "Reading file: Final_Dataset/Vishnu/RK.csv\n",
      "Reading file: Final_Dataset/Vishnu/RA.csv\n",
      "Reading file: Final_Dataset/Vishnu/LA.csv\n",
      "Reading file: Final_Dataset/Vinayak/LK.csv\n",
      "Reading file: Final_Dataset/Vinayak/LH.csv\n",
      "Reading file: Final_Dataset/Vinayak/RH.csv\n",
      "Reading file: Final_Dataset/Vinayak/RK.csv\n",
      "Reading file: Final_Dataset/Vinayak/RA.csv\n",
      "Reading file: Final_Dataset/Vinayak/LA.csv\n",
      "Reading file: Final_Dataset/Ragesh/LK.csv\n",
      "Reading file: Final_Dataset/Ragesh/LH.csv\n",
      "Reading file: Final_Dataset/Ragesh/RH.csv\n",
      "Reading file: Final_Dataset/Ragesh/RK.csv\n",
      "Reading file: Final_Dataset/Ragesh/RA.csv\n",
      "Reading file: Final_Dataset/Ragesh/LA.csv\n",
      "Reading file: Final_Dataset/Krishnanand/LK.csv\n",
      "Reading file: Final_Dataset/Krishnanand/LH.csv\n",
      "Reading file: Final_Dataset/Krishnanand/RH.csv\n",
      "Reading file: Final_Dataset/Krishnanand/RK.csv\n",
      "Reading file: Final_Dataset/Krishnanand/RA.csv\n",
      "Reading file: Final_Dataset/Krishnanand/LA.csv\n",
      "Reading file: Final_Dataset/Me/LK.csv\n",
      "Reading file: Final_Dataset/Me/LH.csv\n",
      "Reading file: Final_Dataset/Me/RH.csv\n",
      "Reading file: Final_Dataset/Me/RK.csv\n",
      "Reading file: Final_Dataset/Me/RA.csv\n",
      "Reading file: Final_Dataset/Me/LA.csv\n",
      "Reading file: Final_Dataset/Ashna/LK.csv\n",
      "Reading file: Final_Dataset/Ashna/LH.csv\n",
      "Reading file: Final_Dataset/Ashna/RH.csv\n",
      "Reading file: Final_Dataset/Ashna/RK.csv\n",
      "Reading file: Final_Dataset/Ashna/RA.csv\n",
      "Reading file: Final_Dataset/Ashna/LA.csv\n",
      "Reading file: Final_Dataset/SreeMegha/LK.csv\n",
      "Reading file: Final_Dataset/SreeMegha/LH.csv\n",
      "Reading file: Final_Dataset/SreeMegha/RH.csv\n",
      "Reading file: Final_Dataset/SreeMegha/RK.csv\n",
      "Reading file: Final_Dataset/SreeMegha/RA.csv\n",
      "Reading file: Final_Dataset/SreeMegha/LA.csv\n"
     ]
    }
   ],
   "source": [
    "base_dir = 'Final_Dataset'  # Update this to your base path\n",
    "parsed_data = load_and_parse_data(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import median_filter, gaussian_filter\n",
    "\n",
    "# Apply Median and Gaussian filters\n",
    "def median_then_gaussian_filter(data, kernel_size=5, sigma=1):\n",
    "    median_filtered = median_filter(data, size=kernel_size)\n",
    "    gaussian_filtered = gaussian_filter(median_filtered, sigma=sigma)\n",
    "    return gaussian_filtered\n",
    "\n",
    "def preprocess_data(data):\n",
    "    for subject in data:\n",
    "        for joint in data[subject]:\n",
    "            df = data[subject][joint]\n",
    "            filtered_data = {}\n",
    "            for column in df.columns[1:]:  # Skip 'Time (s)'\n",
    "                filtered_data[f'{column}_filtered'] = median_then_gaussian_filter(df[column].values)\n",
    "            df_filtered = pd.DataFrame(filtered_data)\n",
    "            df_filtered['Time (s)'] = df['Time (s)']\n",
    "            data[subject][joint] = df_filtered\n",
    "    return data\n",
    "\n",
    "# Preprocess the data\n",
    "parsed_data = preprocess_data(parsed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Function to create sequences for LSTM\n",
    "def create_sequences(data, time_steps=50):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i+time_steps])\n",
    "        y.append(data[i+time_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Prepare the data for each joint\n",
    "time_steps = 50\n",
    "X, y = {}, {}\n",
    "\n",
    "for subject in parsed_data:\n",
    "    X[subject], y[subject] = {}, {}\n",
    "    for joint in parsed_data[subject]:\n",
    "        df = parsed_data[subject][joint]\n",
    "        columns_filtered = [col for col in df.columns if col != 'Time (s)']\n",
    "        data_arr = df[columns_filtered].values\n",
    "        X[subject][joint], y[subject][joint] = create_sequences(data_arr, time_steps)\n",
    "\n",
    "# Split into train, validation, and test sets and scale the data\n",
    "train_size = 0.7\n",
    "val_size = 0.15\n",
    "test_size = 0.15\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = {}, {}, {}, {}, {}, {}\n",
    "scalers = {}\n",
    "imputers = {}\n",
    "\n",
    "for subject in X:\n",
    "    X_train[subject], X_val[subject], X_test[subject] = {}, {}, {}\n",
    "    y_train[subject], y_val[subject], y_test[subject] = {}, {}, {}\n",
    "    scalers[subject] = {}\n",
    "    imputers[subject] = {}\n",
    "    \n",
    "    for joint in X[subject]:\n",
    "        n_train = int(len(X[subject][joint]) * train_size)\n",
    "        n_val = int(len(X[subject][joint]) * val_size)\n",
    "        \n",
    "        X_train[subject][joint] = X[subject][joint][:n_train]\n",
    "        y_train[subject][joint] = y[subject][joint][:n_train]\n",
    "        X_val[subject][joint] = X[subject][joint][n_train:n_train + n_val]\n",
    "        y_val[subject][joint] = y[subject][joint][n_train:n_train + n_val]\n",
    "        X_test[subject][joint] = X[subject][joint][n_train + n_val:]\n",
    "        y_test[subject][joint] = y[subject][joint][n_train + n_val:]\n",
    "        \n",
    "        # Handle NaNs and scale the data\n",
    "        imputers[subject][joint] = SimpleImputer(strategy='mean')\n",
    "        X_train[subject][joint] = imputers[subject][joint].fit_transform(\n",
    "            np.nan_to_num(X_train[subject][joint], nan=0.0).reshape(-1, X_train[subject][joint].shape[-1])\n",
    "        ).reshape(X_train[subject][joint].shape)\n",
    "        \n",
    "        scalers[subject][joint] = StandardScaler()\n",
    "        X_train[subject][joint] = scalers[subject][joint].fit_transform(\n",
    "            X_train[subject][joint].reshape(-1, X_train[subject][joint].shape[-1])\n",
    "        ).reshape(X_train[subject][joint].shape)\n",
    "        \n",
    "        X_val[subject][joint] = imputers[subject][joint].transform(\n",
    "            np.nan_to_num(X_val[subject][joint], nan=0.0).reshape(-1, X_val[subject][joint].shape[-1])\n",
    "        ).reshape(X_val[subject][joint].shape)\n",
    "        \n",
    "        X_val[subject][joint] = scalers[subject][joint].transform(\n",
    "            X_val[subject][joint].reshape(-1, X_val[subject][joint].shape[-1])\n",
    "        ).reshape(X_val[subject][joint].shape)\n",
    "        \n",
    "        X_test[subject][joint] = imputers[subject][joint].transform(\n",
    "            np.nan_to_num(X_test[subject][joint], nan=0.0).reshape(-1, X_test[subject][joint].shape[-1])\n",
    "        ).reshape(X_test[subject][joint].shape)\n",
    "        \n",
    "        X_test[subject][joint] = scalers[subject][joint].transform(\n",
    "            X_test[subject][joint].reshape(-1, X_test[subject][joint].shape[-1])\n",
    "        ).reshape(X_test[subject][joint].shape)\n",
    "\n",
    "print(\"Data preprocessing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Function to create LSTM model\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=input_shape))\n",
    "    model.add(LSTM(50, activation='relu'))\n",
    "    model.add(Dense(input_shape[-1]))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Train the model for each joint\n",
    "models = {}\n",
    "\n",
    "for subject in X_train:\n",
    "    models[subject] = {}\n",
    "    for joint in X_train[subject]:\n",
    "        print(f'Training model for {subject} - {joint}')\n",
    "        model = create_lstm_model((X_train[subject][joint].shape[1], X_train[subject][joint].shape[2]))\n",
    "        model.fit(X_train[subject][joint], y_train[subject][joint], epochs=50, batch_size=32, validation_data=(X_val[subject][joint], y_val[subject][joint]), verbose=1)\n",
    "        models[subject][joint] = model\n",
    "        model.save(f'{subject}_{joint}_lstm_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate the model for each joint\n",
    "for subject in models:\n",
    "    for joint in models[subject]:\n",
    "        print(f'Evaluating model for {subject} - {joint}')\n",
    "        model = models[subject][joint]\n",
    "        predictions = model.predict(X_test[subject][joint])\n",
    "        mse = mean_squared_error(y_test[subject][joint], predictions)\n",
    "        mae = mean_absolute_error(y_test[subject][joint], predictions)\n",
    "        print(f'{subject} - {joint} - Mean Squared Error: {mse}')\n",
    "        print(f'{subject} - {joint} - Mean Absolute Error: {mae}')\n",
    "\n",
    "        # Plot some test results for visual inspection\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(y_test[subject][joint][:100, 0], label='True')\n",
    "        plt.plot(predictions[:100, 0], label='Predicted')\n",
    "        plt.title(f'True vs Predicted values for {subject} - {joint} - first feature')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
