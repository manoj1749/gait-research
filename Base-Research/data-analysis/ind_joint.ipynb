{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.ndimage import median_filter, gaussian_filter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data from a CSV file\n",
    "def load_data(file_path):\n",
    "    column_names = ['Time (s)', 'X (m/s2)', 'Y (m/s2)', 'Z (m/s2)', 'R (m/s2)', 'Theta (deg)', 'Phi (deg)']\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()[4:]  # Skip metadata lines\n",
    "        data = [line.strip().split(',') for line in lines if len(line.strip().split(',')) == len(column_names)]\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')  # Convert to numeric\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Median and Gaussian filters\n",
    "def median_then_gaussian_filter(data, kernel_size=5, sigma=1):\n",
    "    median_filtered = median_filter(data, size=kernel_size)\n",
    "    gaussian_filtered = gaussian_filter(median_filtered, sigma=sigma)\n",
    "    return gaussian_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    filtered_data = {}\n",
    "    for column in df.columns[1:]:  # Skip 'Time (s)'\n",
    "        filtered_data[f'{column}_filtered'] = median_then_gaussian_filter(df[column].values)\n",
    "    df_filtered = pd.DataFrame(filtered_data)\n",
    "    df_filtered['Time (s)'] = df['Time (s)']\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences for LSTM\n",
    "def create_sequences(data, time_steps=50):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i+time_steps])\n",
    "        y.append(data[i+time_steps])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data into train, validation, and test sets\n",
    "def split_data(X, y, train_size=0.7, val_size=0.15, test_size=0.15):\n",
    "    n_train = int(len(X) * train_size)\n",
    "    n_val = int(len(X) * val_size)\n",
    "    \n",
    "    X_train = X[:n_train]\n",
    "    y_train = y[:n_train]\n",
    "    X_val = X[n_train:n_train + n_val]\n",
    "    y_val = y[n_train:n_train + n_val]\n",
    "    X_test = X[n_train + n_val:]\n",
    "    y_test = y[n_train + n_val:]\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create LSTM model\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=input_shape))\n",
    "    model.add(LSTM(50, activation='relu'))\n",
    "    model.add(Dense(input_shape[-1]))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate_model(file_path, time_steps=50, epochs=50, batch_size=32):\n",
    "    # Load and preprocess the data\n",
    "    df = load_data(file_path)\n",
    "    df_filtered = preprocess_data(df)\n",
    "    \n",
    "    # Create sequences\n",
    "    columns_filtered = [col for col in df_filtered.columns if col != 'Time (s)']\n",
    "    data_arr = df_filtered[columns_filtered].values\n",
    "    X, y = create_sequences(data_arr, time_steps)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)\n",
    "    \n",
    "    # Handle NaNs and scale the data\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train = imputer.fit_transform(np.nan_to_num(X_train, nan=0.0).reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "    \n",
    "    X_val = imputer.transform(np.nan_to_num(X_val, nan=0.0).reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
    "    X_val = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
    "    \n",
    "    X_test = imputer.transform(np.nan_to_num(X_test, nan=0.0).reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "    X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = create_lstm_model((X_train.shape[1], X_train.shape[-1]))\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val), verbose=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    predictions = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    print(f'Mean Squared Error: {mse}')\n",
    "    print(f'Mean Absolute Error: {mae}')\n",
    "    \n",
    "    # Plot some test results for visual inspection\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_test[:100, 0], label='True')\n",
    "    plt.plot(predictions[:100, 0], label='Predicted')\n",
    "    plt.title('True vs Predicted values - first feature')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the CSV file path here\n",
    "file_path = 'Final_Dataset/Abhay/RK.csv'  # Update with your CSV file path\n",
    "model = train_and_evaluate_model(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
